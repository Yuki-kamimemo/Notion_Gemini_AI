import streamlit as st
from ddgs import DDGS
import trafilatura
import traceback
import httpx
import io
import pdfplumber
import docx

from notion_utils import notion_blocks_to_markdown, markdown_to_notion_blocks

def process_uploaded_files(uploaded_files):
    # (ã“ã®é–¢æ•°ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“)
    full_text = ""
    if not uploaded_files:
        return full_text
    for uploaded_file in uploaded_files:
        full_text += f"--- å‚è€ƒè³‡æ–™: {uploaded_file.name} ---\n\n"
        try:
            if uploaded_file.name.lower().endswith('.pdf'):
                with pdfplumber.open(uploaded_file) as pdf:
                    for page in pdf.pages:
                        text = page.extract_text()
                        if text:
                            full_text += text + "\n"
            elif uploaded_file.name.lower().endswith('.docx'):
                document = docx.Document(uploaded_file)
                for para in document.paragraphs:
                    full_text += para.text + "\n"
            elif uploaded_file.name.lower().endswith('.txt'):
                stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
                full_text += stringio.read() + "\n"
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        full_text += "\n\n"
    return full_text

def get_content_from_single_url(url: str, status_placeholder):
    # (ã“ã®é–¢æ•°ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“)
    status_placeholder.info(f"å˜ä¸€URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™: {url}")
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0',
    }
    try:
        with httpx.Client(headers=HEADERS, follow_redirects=True, timeout=15.0) as client:
            response = client.get(url)
            response.raise_for_status() 
            extracted = trafilatura.extract(response.text, include_comments=False, include_tables=True)
            if extracted:
                return f"--- å‚è€ƒURL: {url} ---\n\n{extracted}"
            else:
                st.error(f"URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¨˜äº‹å½¢å¼ã§ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚: {url}")
                return None
    except Exception as e:
        st.error(f"URLã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {url}\nåŸå› : {e}")
        return None

# --- generate_content_from_web é–¢æ•°ã‚’å¤§å¹…ã«ä¿®æ­£ ---
def generate_content_from_web(user_prompt: str, search_count: int, full_text_token_limit: int, status_placeholder, results_placeholder):
    status_placeholder.info("1/5: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    keyword_prompt = f"ä»¥ä¸‹ã®ã€Œãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–‡ã€ã‹ã‚‰ã€Webæ¤œç´¢ã«ä½¿ã†ã¹ãæœ€ã‚‚é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå›ºæœ‰åè©ãªã©ï¼‰ã‚’æœ€å¤§5ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚\n\nãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–‡ï¼š{user_prompt}\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š"
    keyword_response = st.session_state.gemini_lite_model.generate_content(keyword_prompt)
    search_keywords = keyword_response.text.strip().replace("\n", "")
    if not search_keywords:
        search_keywords = user_prompt
    search_query = f"{search_keywords} -filetype:pdf"
    status_placeholder.info(f"2/5: ã€Œ{search_query}ã€ã§Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
    search_results = list(DDGS().text(search_query, max_results=search_count))
    if not search_results:
        st.error("Webæ¤œç´¢ã§æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, None
    with results_placeholder.container():
        st.info(f"ğŸ¤– **æ¤œç´¢ã‚¯ã‚¨ãƒª (PDFé™¤å¤–):** `{search_query}`")
        with st.expander(f"å‚è€ƒã«ã—ãŸWebã‚µã‚¤ãƒˆ ({len(search_results)}ä»¶)"):
            for result in search_results:
                st.markdown(f"- [{result.get('title')}]({result.get('href')})")
    
    status_placeholder.info("3/5: Webãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0'}
    extracted_articles = []
    with httpx.Client(headers=HEADERS, follow_redirects=True, timeout=15.0) as client:
        for i, result in enumerate(search_results):
            url = result.get('href')
            if url:
                try:
                    response = client.get(url)
                    response.raise_for_status()
                    extracted = trafilatura.extract(response.text, include_comments=False, include_tables=True)
                    if extracted:
                        extracted_articles.append({"url": url, "text": extracted})
                    else:
                        st.warning(f"  - [{i+1}/{len(search_results)}] æœ¬æ–‡æŠ½å‡ºå¤±æ•—: {url}")
                except Exception as e:
                    st.warning(f"  - [{i+1}/{len(search_results)}] URLå‡¦ç†å¤±æ•—: {url}\n  - åŸå› : {e}")
                    continue
    if not extracted_articles:
        st.error("ã©ã®Webã‚µã‚¤ãƒˆã‹ã‚‰ã‚‚è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        return None, None
    
    # --- ã“ã“ã‹ã‚‰ãŒãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
    status_placeholder.info("4/5: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ç®¡ç†ã—ãªãŒã‚‰å‚è€ƒæƒ…å ±ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
    final_context = ""
    current_tokens = 0
    remaining_articles_text = ""

    for i, article in enumerate(extracted_articles):
        article_text_with_header = f"--- å‚è€ƒè¨˜äº‹ {i+1} ({article['url']}) ---\n{article['text']}\n\n"
        
        # ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸Šé™å†…ã§ã‚ã‚Œã°ã€å…¨æ–‡ã‚’è¿½åŠ 
        if current_tokens < full_text_token_limit:
            final_context += article_text_with_header
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¦‚ç®—ï¼ˆæ–‡å­—æ•° / 2ï¼‰ã§æ›´æ–°ã€‚æ­£ç¢ºã§ã¯ãªã„ãŒé€Ÿåº¦å„ªå…ˆã€‚
            current_tokens += len(article_text_with_header) / 2 
        # ä¸Šé™ã‚’è¶…ãˆãŸã‚‰ã€æ®‹ã‚Šã®è¨˜äº‹ã¯ã€Œè¦ç´„å¯¾è±¡ã€ã¨ã—ã¦ã¾ã¨ã‚ã‚‹
        else:
            remaining_articles_text += article_text_with_header
    
    # è¦ç´„å¯¾è±¡ã®è¨˜äº‹ãŒä¸€ã¤ã§ã‚‚ã‚ã‚Œã°ã€ä¸€åº¦ã ã‘è¦ç´„APIã‚’å‘¼ã³å‡ºã™
    if remaining_articles_text:
        status_placeholder.info("ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’è¶…ãˆãŸãŸã‚ã€æ®‹ã‚Šã®è¨˜äº‹ã‚’è¦ç´„ã—ã¦ã„ã¾ã™...")
        summarize_prompt = f"ä»¥ä¸‹ã®è¤‡æ•°ã®è¨˜äº‹ç¾¤ã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æ²¿ã†ã‚ˆã†ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ä¸€ã¤ã®æ–‡ç« ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {user_prompt}\n\n--- è¨˜äº‹ç¾¤ ---\n{remaining_articles_text}"
        try:
            summary_response = st.session_state.gemini_lite_model.generate_content(summarize_prompt)
            final_context += f"--- è¤‡æ•°ã®å‚è€ƒè¨˜äº‹ã®è¦ç´„ ---\n{summary_response.text}\n\n"
            st.info("æ®‹ã‚Šã®è¨˜äº‹ã®è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.warning(f"è¦ç´„å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    # --- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã“ã“ã¾ã§ ---

    status_placeholder.info("5/5: Geminiã«ã‚ˆã‚‹æœ€çµ‚çš„ãªè¨˜äº‹ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    return final_context

# core_logic.py ã®ä¸­

def parse_gemini_output(text, fallback_prompt):
    """
    Geminiã®å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    æœŸå¾…é€šã‚Šã®å½¢å¼ã§ãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚
    """
    # æœ€åˆã«å¿…ãšãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å®šç¾©ã—ã¦ãŠã
    title = f"ç”Ÿæˆè¨˜äº‹: {fallback_prompt[:20]}..."
    content = ""

    # AIã®å¿œç­”ã«æœŸå¾…ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if "ã‚¿ã‚¤ãƒˆãƒ«ï¼š" in text and "æœ¬æ–‡ï¼š" in text:
        try:
            # "æœ¬æ–‡ï¼š" ã‚’åŸºæº–ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
            title_part, content_part = text.split("æœ¬æ–‡ï¼š", 1)
            # ã‚¿ã‚¤ãƒˆãƒ«éƒ¨åˆ†ã‹ã‚‰ "ã‚¿ã‚¤ãƒˆãƒ«ï¼š" ã‚’å‰Šé™¤ã—ã¦æ•´å½¢
            title = title_part.replace("ã‚¿ã‚¤ãƒˆãƒ«ï¼š", "").strip()
            # æœ¬æ–‡éƒ¨åˆ†ã‚’æ•´å½¢
            content = content_part.strip()
        except Exception:
            # åˆ†å‰²ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å¿œç­”å…¨ä½“ã‚’æœ¬æ–‡ã¨ã—ã¦æ‰±ã†
            content = text
    else:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã‚‚ã€å¿œç­”å…¨ä½“ã‚’æœ¬æ–‡ã¨ã—ã¦æ‰±ã†
        content = text
    
    # titleã¨contentãŒå¿…ãšå®šç¾©ã•ã‚ŒãŸçŠ¶æ…‹ã§å€¤ã‚’è¿”ã™
    return title, content

# ... ã“ã‚Œä»¥é™ã® run_new_page_process ãªã©ã®é–¢æ•°ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“ ...
# --- run_new_page_process é–¢æ•°ã‚’ä¿®æ­£ ---
def run_new_page_process(database_id, user_prompt, ai_persona, uploaded_files, source_url, search_count, full_text_token_limit, status_placeholder, results_placeholder):
    try:
        full_text_context = ""
        if uploaded_files:
            status_placeholder.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            full_text_context = process_uploaded_files(uploaded_files)
        elif source_url:
            full_text_context = get_content_from_single_url(source_url, status_placeholder)
        else:
            full_text_context = generate_content_from_web(user_prompt, search_count, full_text_token_limit, status_placeholder, results_placeholder)
        if not full_text_context:
            st.error("å‚è€ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            return
        # ... (ã“ã‚Œä»¥é™ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—) ...
        final_prompt = f'''
# å‘½ä»¤
{ai_persona} ä¸ãˆã‚‰ã‚ŒãŸã€Œå‚è€ƒæƒ…å ±ã€ã¨ã€Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã«åŸºã¥ãã€é­…åŠ›çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯å¿…ãšã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šï½ã€ã€Œæœ¬æ–‡ï¼šï½ã€ã®å½¢å¼ã§ã€æœ¬æ–‡ã¯Notionã§è¡¨ç¤ºå¯èƒ½ãªMarkdownå½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
è¦‹å‡ºã—ã€ç®‡æ¡æ›¸ãã€**å¤ªå­—**ãªã©ã‚’æ´»ç”¨ã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
# å‚è€ƒæƒ…å ±
{full_text_context}
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
{user_prompt}
# å‡ºåŠ›å½¢å¼ (***å¿…ãšå³å®ˆ***)
ã‚¿ã‚¤ãƒˆãƒ«ï¼š(ã“ã“ã«è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨˜è¿°)
æœ¬æ–‡ï¼š(ã“ã“ã«ä¸Šè¨˜ã®æ›¸å¼ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ãŸNotionè¨˜æ³•ã®Markdownã§è¨˜äº‹ã®æœ¬æ–‡ã‚’è¨˜è¿°)
'''
        response = st.session_state.gemini_model.generate_content(final_prompt)
        text = response.text
        title, content = parse_gemini_output(text, user_prompt)
        with results_placeholder.container(border=True):
            st.markdown(f"### ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {title}")
            st.markdown(content)
            st.info("ä¸Šè¨˜ã®å†…å®¹ã§Notionã«æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’ä½œæˆã—ã¾ã™ã€‚")
        status_placeholder.info("Notionã«æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’ä½œæˆä¸­...")
        all_blocks = markdown_to_notion_blocks(content)
        db_info = st.session_state.notion_client.databases.retrieve(database_id=database_id)
        title_prop_name = next((k for k, v in db_info['properties'].items() if v['type'] == 'title'), 'Name')
        parent_payload = {"database_id": database_id}
        properties_payload = {title_prop_name: {"title": [{"text": {"content": title}}]}}
        created_page = st.session_state.notion_client.pages.create(parent=parent_payload, properties=properties_payload, children=all_blocks[:100])
        if len(all_blocks) > 100:
            page_id = created_page['id']
            for i in range(100, len(all_blocks), 100):
                chunk = all_blocks[i:i+100]
                st.session_state.notion_client.blocks.children.append(block_id=page_id, children=chunk)
        st.balloons()
        status_placeholder.success(f"âœ… æ–°è¦ãƒšãƒ¼ã‚¸ã€Œ{title}ã€ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        st.error(f"âŒ æ–°è¦ãƒšãƒ¼ã‚¸ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.code(traceback.format_exc())


# --- run_edit_page_process é–¢æ•°ã‚’ä¿®æ­£ ---
def run_edit_page_process(page_id, user_prompt, ai_persona, uploaded_files, source_url, search_count, full_text_token_limit, status_placeholder, results_placeholder):
    try:
        status_placeholder.info("1/4: Notionã‹ã‚‰æ—¢å­˜ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        existing_blocks_response = st.session_state.notion_client.blocks.children.list(block_id=page_id)
        existing_markdown = notion_blocks_to_markdown(existing_blocks_response.get('results', []), st.session_state.notion_client)
        with results_placeholder.container(border=True):
            with st.expander("ç¾åœ¨ã®ãƒšãƒ¼ã‚¸å†…å®¹ï¼ˆMarkdownï¼‰"):
                st.markdown(existing_markdown or "ï¼ˆã“ã®ãƒšãƒ¼ã‚¸ã¯ç©ºã§ã™ï¼‰")
        
        full_text_context = ""
        if uploaded_files:
            status_placeholder.info("2/4: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            full_text_context = process_uploaded_files(uploaded_files)
        elif source_url:
            status_placeholder.info("2/4: å˜ä¸€URLã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
            full_text_context = get_content_from_single_url(source_url, status_placeholder)
        else:
            status_placeholder.info("2/4: Webã‹ã‚‰ã®æƒ…å ±åé›†ã‚’é–‹å§‹ã—ã¾ã™...")
            full_text_context = generate_content_from_web(user_prompt, search_count, full_text_token_limit, status_placeholder, results_placeholder)
        
        if not full_text_context:
            st.error("å‚è€ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            return
        
        status_placeholder.info("3/4: AIã«ã‚ˆã‚‹è¿½è¨˜ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
        final_prompt = f'''
# å‘½ä»¤
{ai_persona} ä»¥ä¸‹ã®ã€Œæ—¢å­˜ã®è¨˜äº‹ã€ã¨ã€Œå‚è€ƒæƒ…å ±ã€ã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã€Œè¿½è¨˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã«çš„ç¢ºã«ç­”ãˆã‚‹å½¢ã§ã€**è¿½è¨˜ã™ã¹ãæ–°ã—ã„æ–‡ç« ã®ã¿**ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ—¢å­˜ã®è¨˜äº‹ã®å†…å®¹ã‚’ç¹°ã‚Šè¿”ã™å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# æ—¢å­˜ã®è¨˜äº‹
{existing_markdown}
# å‚è€ƒæƒ…å ±
{full_text_context}
# è¿½è¨˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
{user_prompt}
# å‡ºåŠ›å½¢å¼ (***å¿…ãšå³å®ˆ***)
ã‚¿ã‚¤ãƒˆãƒ«ï¼š(ã“ã“ã«æ—¢å­˜ã®è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€ã¾ãŸã¯æ–°ã—ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨˜è¿°)
æœ¬æ–‡ï¼š(ã“ã“ã«**è¿½è¨˜ã™ã¹ãæ–°ã—ã„æ–‡ç« **ã‚’Markdownå½¢å¼ã§è¨˜è¿°)
'''
        # ... (ã“ã‚Œä»¥é™ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—) ...
        response = st.session_state.gemini_model.generate_content(final_prompt)
        text = response.text
        title, content = parse_gemini_output(text, user_prompt)
        with results_placeholder.container(border=True):
            st.markdown(f"### ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆè¿½è¨˜éƒ¨åˆ†ï¼‰: {title}")
            st.markdown(content)
            st.info("ä¸Šè¨˜ã®å†…å®¹ã‚’Notionãƒšãƒ¼ã‚¸ã®æœ«å°¾ã«è¿½è¨˜ã—ã¾ã™ã€‚")
        status_placeholder.info("4/4: Notionãƒšãƒ¼ã‚¸ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½è¨˜ä¸­...")
        try:
            page_info = st.session_state.notion_client.pages.retrieve(page_id=page_id)
            db_id = page_info.get('parent', {}).get('database_id')
            if db_id:
                db_info = st.session_state.notion_client.databases.retrieve(database_id=db_id)
                title_prop_name = next((k for k, v in db_info['properties'].items() if v['type'] == 'title'), None)
                if title_prop_name:
                    st.session_state.notion_client.pages.update(page_id=page_id, properties={title_prop_name: {"title": [{"text": {"content": title}}]}})
        except Exception as e:
            st.warning(f"ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        new_blocks = markdown_to_notion_blocks(content)
        if new_blocks:
            for i in range(0, len(new_blocks), 100):
                st.session_state.notion_client.blocks.children.append(block_id=page_id, children=new_blocks[i:i+100])
        st.balloons()
        status_placeholder.success(f"âœ… ãƒšãƒ¼ã‚¸ã€Œ{title}ã€ã¸ã®è¿½è¨˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        st.error(f"âŒ ãƒšãƒ¼ã‚¸è¿½è¨˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.code(traceback.format_exc())