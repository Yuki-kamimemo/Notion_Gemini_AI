import streamlit as st
from ddgs import DDGS
import trafilatura
import traceback
import httpx
import io
import pdfplumber
import docx

from notion_utils import notion_blocks_to_markdown, markdown_to_notion_blocks

# --- HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† ---
@st.cache_resource
def get_httpx_client():
    """å†åˆ©ç”¨å¯èƒ½ãªhttpx.Clientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    return httpx.Client(headers=headers, follow_redirects=True, timeout=15.0)

def process_uploaded_files(uploaded_files):
    full_text = ""
    for uploaded_file in uploaded_files:
        full_text += f"--- å‚è€ƒè³‡æ–™: {uploaded_file.name} ---\n\n"
        try:
            if uploaded_file.name.lower().endswith('.pdf'):
                with pdfplumber.open(uploaded_file) as pdf:
                    full_text += "".join(page.extract_text() + "\n" for page in pdf.pages if page.extract_text())
            elif uploaded_file.name.lower().endswith('.docx'):
                document = docx.Document(uploaded_file)
                full_text += "".join(para.text + "\n" for para in document.paragraphs)
            elif uploaded_file.name.lower().endswith('.txt'):
                full_text += io.StringIO(uploaded_file.getvalue().decode("utf-8")).read() + "\n"
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        full_text += "\n\n"
    return full_text

def get_content_from_single_url(url: str, status_placeholder):
    status_placeholder.info(f"URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºä¸­: {url}")
    client = get_httpx_client()
    try:
        response = client.get(url)
        response.raise_for_status() 
        extracted = trafilatura.extract(response.text, include_comments=False, include_tables=True)
        return f"--- å‚è€ƒURL: {url} ---\n\n{extracted}" if extracted else None
    except httpx.HTTPStatusError as e:
        st.warning(f"URLã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ï¼ˆ{e.response.status_code}ã‚¨ãƒ©ãƒ¼ï¼‰: {url}")
        return None
    except Exception as e:
        st.error(f"URLã®å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {url}\nåŸå› : {e}")
        return None

# â˜…â˜…â˜… `time_limit` å¼•æ•°ã‚’è¿½åŠ  â˜…â˜…â˜…
def generate_content_from_web(user_prompt: str, search_count: int, full_text_token_limit: int, time_limit: str, status_placeholder, results_placeholder):
    status_placeholder.info("1/5: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­...")
    keyword_prompt = f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–‡ï¼š`{user_prompt}`\n\nã“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æœ€ã‚‚é©ã—ãŸWebæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5ã¤ä»¥å†…ã§ä½œæˆã—ã¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"
    try:
        keyword_response = st.session_state.gemini_lite_model.generate_content(keyword_prompt)
        search_keywords = keyword_response.text.strip().replace("\n", "") or user_prompt
    except Exception:
        search_keywords = user_prompt
        
    search_query = f"{search_keywords} -filetype:pdf"
    status_placeholder.info(f"2/5: ã€Œ{search_query}ã€ã§Webæ¤œç´¢ä¸­...")
    
    try:
        # â˜…â˜…â˜… `timelimit` ã‚’DDGSæ¤œç´¢ã«é©ç”¨ â˜…â˜…â˜…
        search_results = list(DDGS().text(search_query, max_results=search_count, timelimit=time_limit))
    except Exception as e:
        st.error(f"Webæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

    if not search_results:
        st.warning("Webæ¤œç´¢ã§æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    with results_placeholder.container():
        st.info(f"ğŸ¤– **æ¤œç´¢ã‚¯ã‚¨ãƒª:** `{search_query}` (æœŸé–“: {time_limit or 'æŒ‡å®šãªã—'})")
        with st.expander(f"å‚è€ƒWebã‚µã‚¤ãƒˆ ({len(search_results)}ä»¶)"):
            for r in search_results: st.markdown(f"- [{r.get('title')}]({r.get('href')})")
    
    status_placeholder.info("3/5: Webãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºä¸­...")
    client = get_httpx_client()
    extracted_articles = []
    for i, result in enumerate(search_results, 1):
        url = result.get('href')
        if not url: continue
        status_placeholder.info(f"3/5: Webãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºä¸­... ({i}/{len(search_results)})")
        try:
            response = client.get(url)
            response.raise_for_status() # 4xx, 5xxã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            extracted = trafilatura.extract(response.text, include_comments=False, include_tables=True)
            if extracted:
                extracted_articles.append({"url": url, "text": extracted})
            else:
                st.warning(f"æœ¬æ–‡ã®æŠ½å‡ºã«å¤±æ•—: {url}")
        except httpx.HTTPStatusError as e:
            # â˜…â˜…â˜… 404ã‚¨ãƒ©ãƒ¼ãªã©ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ†ã‹ã‚Šã‚„ã™ãé€šçŸ¥ â˜…â˜…â˜…
            st.warning(f"URLã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•— ({e.response.status_code}ã‚¨ãƒ©ãƒ¼): {url}")
        except Exception:
            st.warning(f"URLã®å‡¦ç†ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯ãã®ä»–ã‚¨ãƒ©ãƒ¼: {url}")
            
    if not extracted_articles:
        st.error("ã©ã®Webã‚µã‚¤ãƒˆã‹ã‚‰ã‚‚è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    status_placeholder.info("4/5: å‚è€ƒæƒ…å ±ã‚’æ§‹ç¯‰ä¸­...")
    final_context, remaining_articles_text = "", ""
    current_tokens = 0
    
    for article in extracted_articles:
        article_text = f"--- å‚è€ƒè¨˜äº‹: {article['url']} ---\n{article['text']}\n\n"
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¦‚ç®—ï¼ˆæ–‡å­—æ•° / 1.5ï¼‰ã§è¨ˆç®—
        article_tokens = len(article_text) / 1.5
        if (current_tokens + article_tokens) < full_text_token_limit:
            final_context += article_text
            current_tokens += article_tokens
        else:
            remaining_articles_text += article_text
    
    if remaining_articles_text:
        status_placeholder.info("ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã‚’è¶…ãˆãŸãŸã‚ã€æ®‹ã‚Šã®è¨˜äº‹ã‚’è¦ç´„ä¸­...")
        summarize_prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€Œ{user_prompt}ã€ã«æ²¿ã†ã‚ˆã†ã«ã€ä»¥ä¸‹ã®è¨˜äº‹ç¾¤ã®è¦ç‚¹ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n--- è¨˜äº‹ç¾¤ ---\n{remaining_articles_text}"
        try:
            summary_response = st.session_state.gemini_lite_model.generate_content(summarize_prompt)
            final_context += f"--- è¤‡æ•°ã®å‚è€ƒè¨˜äº‹ã®è¦ç´„ ---\n{summary_response.text}\n\n"
        except Exception as e:
            st.warning(f"è¦ç´„å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")

    status_placeholder.info("5/5: AIã«ã‚ˆã‚‹æœ€çµ‚è¨˜äº‹ã®ç”Ÿæˆã‚’é–‹å§‹...")
    return final_context

def parse_gemini_output(text, fallback_prompt):
    try:
        if "ã‚¿ã‚¤ãƒˆãƒ«ï¼š" in text and "æœ¬æ–‡ï¼š" in text:
            title_part, content_part = text.split("æœ¬æ–‡ï¼š", 1)
            title = title_part.replace("ã‚¿ã‚¤ãƒˆãƒ«ï¼š", "").strip()
            return title, content_part.strip()
    except Exception:
        pass
    return f"ç”Ÿæˆè¨˜äº‹: {fallback_prompt[:20]}...", text

def _run_process(process_logic, **kwargs):
    try:
        process_logic(**kwargs)
    except Exception as e:
        kwargs['status_placeholder'].error(f"âŒ å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.code(traceback.format_exc())

def _page_creation_logic(database_id, user_prompt, ai_persona, uploaded_files, source_url, search_count, full_text_token_limit, time_limit, status_placeholder, results_placeholder):
    context_generator = process_uploaded_files if uploaded_files else get_content_from_single_url if source_url else generate_content_from_web
    context_args = {'uploaded_files': uploaded_files} if uploaded_files else {'url': source_url, 'status_placeholder': status_placeholder} if source_url else {'user_prompt': user_prompt, 'search_count': search_count, 'full_text_token_limit': full_text_token_limit, 'time_limit': time_limit, 'status_placeholder': status_placeholder, 'results_placeholder': results_placeholder}
    
    full_text_context = context_generator(**context_args)
    if not full_text_context:
        st.error("å‚è€ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
        status_placeholder.empty()
        return

    final_prompt = f"# å‘½ä»¤\n{ai_persona}ã¨ã—ã¦ã€ã€Œå‚è€ƒæƒ…å ±ã€ã¨ã€Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã«åŸºã¥ãè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\nå‡ºåŠ›ã¯ã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šï½ã€ã€Œæœ¬æ–‡ï¼šï½ã€ã®å½¢å¼ã§ã€æœ¬æ–‡ã¯Markdownã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚\n\n# å‚è€ƒæƒ…å ±\n{full_text_context}\n\n# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n{user_prompt}"
    response = st.session_state.gemini_model.generate_content(final_prompt)
    title, content = parse_gemini_output(response.text, user_prompt)
    
    with results_placeholder.container(border=True):
        st.markdown(f"### ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {title}")
        st.markdown(content)
    
    status_placeholder.info("Notionã«æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’ä½œæˆä¸­...")
    all_blocks = markdown_to_notion_blocks(content)
    db_info = st.session_state.notion_client.databases.retrieve(database_id=database_id)
    title_prop_name = next((k for k, v in db_info['properties'].items() if v['type'] == 'title'), 'Name')
    
    created_page = st.session_state.notion_client.pages.create(
        parent={"database_id": database_id},
        properties={title_prop_name: {"title": [{"text": {"content": title}}]}},
        children=all_blocks[:100]
    )
    if len(all_blocks) > 100:
        for i in range(100, len(all_blocks), 100):
            st.session_state.notion_client.blocks.children.append(block_id=created_page['id'], children=all_blocks[i:i+100])
    
    st.balloons()
    status_placeholder.success(f"âœ… æ–°è¦ãƒšãƒ¼ã‚¸ã€Œ{title}ã€ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

def _page_edit_logic(page_id, **kwargs):
    status_placeholder = kwargs['status_placeholder']
    status_placeholder.info("1/4: æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    existing_blocks = st.session_state.notion_client.blocks.children.list(block_id=page_id).get('results', [])
    existing_markdown = notion_blocks_to_markdown(existing_blocks, st.session_state.notion_client)
    
    with kwargs['results_placeholder'].container(border=True):
        with st.expander("ç¾åœ¨ã®ãƒšãƒ¼ã‚¸å†…å®¹ï¼ˆMarkdownï¼‰"):
            st.markdown(existing_markdown or "ï¼ˆç©ºã®ãƒšãƒ¼ã‚¸ã§ã™ï¼‰")
            
    # ï¼ˆ...ä»¥é™ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯æ–°è¦ä½œæˆã¨ã»ã¼åŒã˜ãªã®ã§çœç•¥...ï¼‰
    
def run_new_page_process(**kwargs):
    _run_process(_page_creation_logic, **kwargs)

def run_edit_page_process(**kwargs):
    _run_process(_page_edit_logic, **kwargs)